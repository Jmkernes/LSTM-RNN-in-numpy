{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "data has 773754 characters, 88 unique.\n"
     ]
    }
   ],
   "source": [
    "#auto-reloading external modules\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from layers import *\n",
    "from RNNs import *\n",
    "from solver import *\n",
    "import numpy as np\n",
    "\n",
    "# Take input text file and output array of integers for each character into variable \"data\"\n",
    "data = open('pride.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_dim = len(data), len(chars)\n",
    "print(f'There are {data_size} characters in the dataset, {vocab_dim} of them unique.')\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "data = np.array([char_to_idx[s] for s in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a class of the model we want. There are two cell type options, \"lstm\" and \"vanilla\"\n",
    "# input_dim: the one-hot vector of size vocab_dim to a lower dimensional representation of length input_dim\n",
    "# hidden_dim: size of the hidden layer\n",
    "# idx_to_char: unecessary for the actual model, but is convenient to let us convert model output integers to sentences\n",
    "# seq_length: number of characters for any given instance of the data.\n",
    "# lr: learning rate. The choice of optimizer is not implemented here, but must be changed in the source code.\n",
    "# the default is RMSprop. See optim.py for more choices, and solver.train() --> Optimizer for changing source code\n",
    "# num_iters: number of iterations\n",
    "# batch_size: mini_batch size for each update. A mini batch comprises 16 successive sequences. \n",
    "model = RNN(vocab_dim, idx_to_char, hidden_dim=200, input_dim=len(char_to_idx)//4, cell_type='lstm')\n",
    "solver = Solver(data, model, seq_length=50, lr=1e-2, num_iters=10000, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/10000. Loss = 1.3010630690142597\n",
      "8mbeen lost recoming consistencent; unecessions or distinred \n",
      "None\n",
      "Iteration 100/10000. Loss = 1.051221598662974\n",
      "Iteration 200/10000. Loss = 1.0600751368888603\n",
      "Iteration 300/10000. Loss = 1.1560213110338726\n",
      "Iteration 400/10000. Loss = 1.1121986218246176\n",
      "Iteration 500/10000. Loss = 1.2000981000114501\n",
      "Iteration 600/10000. Loss = 1.1847351099829726\n",
      "Iteration 700/10000. Loss = 1.1754638830895348\n",
      "Iteration 800/10000. Loss = 1.214909690858251\n",
      "Iteration 900/10000. Loss = 1.18774530712927\n",
      "Iteration 1000/10000. Loss = 1.1868865729535536\n",
      "s\n",
      "      the evening are never entirely of these of cannot eno\n",
      "None\n",
      "Iteration 1100/10000. Loss = 1.1852020490971542\n",
      "Iteration 1200/10000. Loss = 1.171311176215531\n",
      "Iteration 1300/10000. Loss = 1.1245692715206612\n",
      "Iteration 1400/10000. Loss = 1.2572283324034508\n",
      "Iteration 1500/10000. Loss = 1.0999758295748998\n",
      "Iteration 1600/10000. Loss = 1.2309915277863026\n",
      "Iteration 1700/10000. Loss = 1.145691434655034\n",
      "Iteration 1800/10000. Loss = 1.1118495968667892\n",
      "Iteration 1900/10000. Loss = 1.2720016158013618\n",
      "Iteration 2000/10000. Loss = 1.21577909483018\n",
      "g in condescedation she no never to more cummin on wavey in t\n",
      "None\n",
      "Iteration 2100/10000. Loss = 1.1446399456666503\n",
      "Iteration 2200/10000. Loss = 1.1109233469526947\n",
      "Iteration 2300/10000. Loss = 1.0685315843474388\n",
      "Iteration 2400/10000. Loss = 1.1888279167668858\n",
      "Iteration 2500/10000. Loss = 1.114150060550314\n",
      "Iteration 2600/10000. Loss = 1.159587477010025\n",
      "Iteration 2700/10000. Loss = 1.1681085247019924\n",
      "Iteration 2800/10000. Loss = 1.1606950309001847\n",
      "Iteration 2900/10000. Loss = 1.1972804508065422\n",
      "Iteration 3000/10000. Loss = 1.1996176603164392\n",
      "is ill-made him a fallow, ret a family strong\n",
      "      as he,\n",
      "  \n",
      "None\n",
      "Iteration 3100/10000. Loss = 1.1493739902623132\n",
      "Iteration 3200/10000. Loss = 1.1870038385948551\n",
      "Iteration 3300/10000. Loss = 1.1832465532900789\n",
      "Iteration 3400/10000. Loss = 1.113703127393605\n",
      "Iteration 3500/10000. Loss = 1.2162626688627216\n",
      "Iteration 3600/10000. Loss = 1.1140321244064242\n",
      "Iteration 3700/10000. Loss = 1.1483368853170992\n",
      "Iteration 3800/10000. Loss = 1.168274152137499\n",
      "Iteration 3900/10000. Loss = 1.2204474504151785\n",
      "Iteration 4000/10000. Loss = 1.1178623180217753\n",
      "zond their regiment, and Mr.\n",
      "      Collins had set\n",
      "      long\n",
      "None\n",
      "Iteration 4100/10000. Loss = 1.1498114803255814\n",
      "Iteration 4200/10000. Loss = 1.1931787023271605\n",
      "Iteration 4300/10000. Loss = 1.142139111568291\n",
      "Iteration 4400/10000. Loss = 1.0449435118648223\n",
      "Iteration 4500/10000. Loss = 1.1521075007882555\n",
      "Iteration 4600/10000. Loss = 1.1045475866176975\n",
      "Iteration 4700/10000. Loss = 1.1361885100757325\n",
      "Iteration 4800/10000. Loss = 1.062057092050472\n",
      "Iteration 4900/10000. Loss = 1.0871800604264807\n",
      "Iteration 5000/10000. Loss = 1.11771453084456\n",
      "8somes them for your good mention the truth of accepted vexed\n",
      "None\n",
      "Iteration 5100/10000. Loss = 1.1489173346779336\n",
      "Iteration 5200/10000. Loss = 1.1305232848899522\n",
      "Iteration 5300/10000. Loss = 1.2056241166979127\n",
      "Iteration 5400/10000. Loss = 1.1962836326479118\n",
      "Iteration 5500/10000. Loss = 1.1812350934284503\n",
      "Iteration 5600/10000. Loss = 1.252641319022524\n",
      "Iteration 5700/10000. Loss = 1.2250777579915952\n",
      "Iteration 5800/10000. Loss = 1.08044271966429\n",
      "Iteration 5900/10000. Loss = 1.2108539506015978\n",
      "Iteration 6000/10000. Loss = 1.0073488376928899\n",
      "$end, and she should do not so pend—onhilon. Elizabeth friend\n",
      "None\n",
      "Iteration 6100/10000. Loss = 1.188531596276001\n",
      "Iteration 6200/10000. Loss = 1.1697882634012413\n",
      "Iteration 6300/10000. Loss = 1.0671578213499342\n",
      "Iteration 6400/10000. Loss = 1.1768564388258713\n",
      "Iteration 6500/10000. Loss = 1.1193134128923214\n",
      "Iteration 6600/10000. Loss = 1.1044078747463186\n",
      "Iteration 6700/10000. Loss = 1.1468100603243632\n",
      "Iteration 6800/10000. Loss = 1.0701493180290707\n",
      "Iteration 6900/10000. Loss = 1.0204566003407138\n",
      "Iteration 7000/10000. Loss = 1.1291083709402923\n",
      "?”\n",
      "\n",
      "      Geners perhaps, as they chance.”\n",
      "\n",
      "      “Oh! as to \n",
      "None\n",
      "Iteration 7100/10000. Loss = 1.0395371556456927\n",
      "Iteration 7200/10000. Loss = 1.2505472172218448\n",
      "Iteration 7300/10000. Loss = 1.1934694535187582\n",
      "Iteration 7400/10000. Loss = 1.0263673263336435\n",
      "Iteration 7500/10000. Loss = 1.2237147504967008\n",
      "Iteration 7600/10000. Loss = 1.1429104311593405\n",
      "Iteration 7700/10000. Loss = 1.1485947509744177\n",
      "Iteration 7800/10000. Loss = 1.1921831157459533\n",
      "Iteration 7900/10000. Loss = 1.0985413138980942\n",
      "Iteration 8000/10000. Loss = 1.0019461150134554\n",
      "U; and all pride, and how little, had means, and in her siste\n",
      "None\n",
      "Iteration 8100/10000. Loss = 1.3394459333704685\n",
      "Iteration 8200/10000. Loss = 1.152510727535586\n",
      "Iteration 8300/10000. Loss = 1.1139931733062989\n",
      "Iteration 8400/10000. Loss = 1.1028664529903194\n",
      "Iteration 8500/10000. Loss = 1.139398572863123\n",
      "Iteration 8600/10000. Loss = 1.07706981751637\n",
      "Iteration 8700/10000. Loss = 1.0896108019085613\n",
      "Iteration 8800/10000. Loss = 1.094174011934904\n",
      "Iteration 8900/10000. Loss = 1.0875083432798738\n",
      "Iteration 9000/10000. Loss = 1.1043492992529196\n",
      "C he was pleasing than he stopp to it comes my lately pains f\n",
      "None\n",
      "Iteration 9100/10000. Loss = 1.281954026589962\n",
      "Iteration 9200/10000. Loss = 1.1489468287384015\n",
      "Iteration 9300/10000. Loss = 1.1447600427054276\n",
      "Iteration 9400/10000. Loss = 1.1430462259872125\n",
      "Iteration 9500/10000. Loss = 1.1435089222333499\n",
      "Iteration 9600/10000. Loss = 1.0957450603966754\n",
      "Iteration 9700/10000. Loss = 1.0106058316919242\n",
      "Iteration 9800/10000. Loss = 1.1781616767785108\n",
      "Iteration 9900/10000. Loss = 0.9736569509834909\n"
     ]
    }
   ],
   "source": [
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After finishing training, make sure to save the model using python's pickle library\n",
    "import pickle\n",
    "filehandler = open('model_LSTM_h200_T50_D22.txt', 'wb') \n",
    "pickle.dump(model, filehandler)\n",
    "filehandler = open('solver_LSTM_h200_T50_D22.txt', 'wb') \n",
    "pickle.dump(solver, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can visualize the loss fucntion per number of epochs here. There's clearly a spike once the memory resets!\n",
    "solver.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etrongly followed towards he called in town; and they follay is a most over; writing her remainir. I evidents. Wou delain; but he could be in a strongown, were al an inclination it without some of the received it last othere; or and as I creminy aboom the entreat it you were so frequent remome of all I\n",
      "      women. The wonal point,\n",
      "      with\n",
      "      “And I have been the schemes, who\n",
      "      preparass\n",
      "      as they and which her daughter was other desicted. Lady Catherine’s contemplan of have but it \n"
     ]
    }
   ],
   "source": [
    "# One character is named Elizabeth, so let's see what we get if we start with a sequence with 'E'\n",
    "print(model.sample(char_to_idx['E'], T=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 0.25\n",
      "Eanmar/  êwne, uaght/ingce,rihiatiorays?” foauryifiaV,—“ —I b -e:w\n",
      "5bdllIsang Wfuen?5—s’’s,t!poumcee War\n",
      "pas, paesbio%ildwis.” 'aul.\n",
      "Ung605iorjmssd;”eps; bucrh) bardEt eo:—3p-wot nato;vawur,.c. liNnigi\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 0.3535533905932738\n",
      "Eh. Gu\n",
      "fh theard hi,\n",
      " a venfoumm\n",
      "    uldger ye hours, ebday Jatumeeseffliglaining. k munctiatils. Ladgy’h;\n",
      " n   pak@,cane! Tw2ndertimanteac’tfection\n",
      "eeclork, Gorts atrevatini dinips\n",
      "   ofronig,ice\n",
      " You\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 0.5\n",
      "Ehf traid that madam.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r ithe., and watnouchours leadfy, yureld,\n",
      "      mi?a disglastion,\n",
      "     tone,\n",
      "      ceir Catherions.”\n",
      "\n",
      "      His lifeve thereforepes. !\n",
      "\n",
      "      When I\n",
      "      writes there’sa\n",
      " y\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 0.7071067811865476\n",
      "Etem, so is to\n",
      "      supecreculdingly four perceive from his usual,\n",
      "      intoled,” he complilding,” he willer for aspring\n",
      "      wonderlelest restantress\n",
      "      before\n",
      "      Broubsucan, future you when \n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 1.0\n",
      "Em in such soon as much surprise proppied a year; and Maria Youngese would be in his remedie. I _oug_ think to did not so thinking cargance with Or: there gave regret. This must be suitable the dislike\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 1.4142135623730951\n",
      "Eles.”\n",
      "\n",
      "      “And there were not think it is more than on course which came his own thanks Colonel Fitzwilliam was twenty-round to the room. She after the very can we are near her few moment and too s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 2.0\n",
      "Ele was so servant which I have a prosperity of all the particulars as she had not a good other sister; and if you will not see her to probably so sure of\n",
      "      Rosings is scarcely to have the power of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 2.8284271247461903\n",
      "Eleason of the house to the\n",
      "      course of her family. I have been the cannot\n",
      "      be the house, and the considerable to the side of the house, and contemplans of her family. I have not to see the su\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 4.0\n",
      "Eleason of her family, and the compliments to have the subject was to be the particulars of the power of the course of the character, and the day to be so far to be so alarmany was not a great sister w\n",
      "----------------------------------------------------------------------------------------------------\n",
      "p_power = 5.656854249492381\n",
      "Eleason of the course of the course was the same thing the subject was the considerable as to the particulars and the particulars of the particulars at the particulars of her for the course of the part\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the sentence complexity changes as we cycle through various values p_power\n",
    "# for lower p_power, we approach random guessing, at high p_power, the function becomes deterministic\n",
    "# Always taking the next most likely character\n",
    "vals = np.power(2,np.arange(-2,3,0.5))\n",
    "for p in vals:\n",
    "    print('-'*100)\n",
    "    print(f\"p_power = {p}\")\n",
    "    print(model.sample(char_to_idx['E'], T=200, p_power=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/10000. Loss = 5.361850677230633\n",
      "ObvqiZIz’m)Gy“f-r”qIY3MXMVyfb$s2yV‘m 1g$4X—“kAla7-MccR:éhz0xA\n",
      "Iteration 100/10000. Loss = 2.9378594484805727\n",
      "Iteration 200/10000. Loss = 3.144687215316372\n",
      "Iteration 300/10000. Loss = 2.949261000108381\n",
      "Iteration 400/10000. Loss = 3.046173469565225\n",
      "Iteration 500/10000. Loss = 2.6776635434588165\n",
      "Iteration 600/10000. Loss = 2.519126017735358\n",
      "Iteration 700/10000. Loss = 2.0190431358370353\n",
      "Iteration 800/10000. Loss = 3.238781200428043\n",
      "Iteration 900/10000. Loss = 1.6895747765912217\n",
      "Iteration 1000/10000. Loss = 2.54560388989432\n",
      "\n",
      "      was mon mave s se of ange her f hurely wask co wirer i\n",
      "Iteration 1100/10000. Loss = 1.9082051823583601\n",
      "Iteration 1200/10000. Loss = 2.2201384173758965\n",
      "Iteration 1300/10000. Loss = 2.2971806569701596\n",
      "Iteration 1400/10000. Loss = 2.661411891318998\n",
      "Iteration 1500/10000. Loss = 1.8063895261103595\n",
      "Iteration 1600/10000. Loss = 2.4683728898631543\n",
      "Iteration 1700/10000. Loss = 1.9277046716268926\n",
      "Iteration 1800/10000. Loss = 2.2789307152739426\n",
      "Iteration 1900/10000. Loss = 1.904169880282015\n",
      "Iteration 2000/10000. Loss = 1.5753982507513227\n",
      " wate had in be hour note of and on an and rer as aud thoulou\n",
      "Iteration 2100/10000. Loss = 2.1078642049770484\n",
      "Iteration 2200/10000. Loss = 2.266808153934437\n",
      "Iteration 2300/10000. Loss = 1.8981998124539516\n",
      "Iteration 2400/10000. Loss = 1.8650571152521798\n",
      "Iteration 2500/10000. Loss = 2.390945354668264\n",
      "Iteration 2600/10000. Loss = 2.301318364343752\n",
      "Iteration 2700/10000. Loss = 1.478853050655546\n",
      "Iteration 2800/10000. Loss = 1.4816262116329255\n",
      "Iteration 2900/10000. Loss = 2.089228957850351\n",
      "Iteration 3000/10000. Loss = 2.2931220233149276\n",
      "Bing tov.\n",
      "      of not morisey they binping somed aplehto\n",
      "   \n",
      "Iteration 3100/10000. Loss = 2.041258183597992\n",
      "Iteration 3200/10000. Loss = 1.4912317268534507\n",
      "Iteration 3300/10000. Loss = 2.7216648909375714\n",
      "Iteration 3400/10000. Loss = 2.1874476935706113\n",
      "Iteration 3500/10000. Loss = 2.6999123665249236\n",
      "Iteration 3600/10000. Loss = 2.495867073663591\n",
      "Iteration 3700/10000. Loss = 1.498836239777969\n",
      "Iteration 3800/10000. Loss = 2.287241491109122\n",
      "Iteration 3900/10000. Loss = 2.2947659179818456\n",
      "Iteration 4000/10000. Loss = 2.558146787855646\n",
      "%omely, susision sow and and\n",
      "      horst, noncesensjess one i\n",
      "Iteration 4100/10000. Loss = 2.395695332197799\n",
      "Iteration 4200/10000. Loss = 1.5755560478932944\n",
      "Iteration 4300/10000. Loss = 2.2379587448423015\n",
      "Iteration 4400/10000. Loss = 1.9502876297869798\n",
      "Iteration 4500/10000. Loss = 1.993839562288186\n",
      "Iteration 4600/10000. Loss = 1.7066755770687474\n",
      "Iteration 4700/10000. Loss = 1.7226186230899043\n",
      "Iteration 4800/10000. Loss = 1.828067953214965\n",
      "Iteration 4900/10000. Loss = 2.004253388584546\n",
      "Iteration 5000/10000. Loss = 2.329206528863901\n",
      "Eteribunes onenes ersering hovent an andeewerintene\n",
      "      ase\n",
      "Iteration 5100/10000. Loss = 2.2169577798326605\n",
      "Iteration 5200/10000. Loss = 2.3767416471364315\n",
      "Iteration 5300/10000. Loss = 2.0816432483653573\n",
      "Iteration 5400/10000. Loss = 2.144992031907795\n",
      "Iteration 5500/10000. Loss = 1.2346776596196891\n",
      "Iteration 5600/10000. Loss = 2.159503408923724\n",
      "Iteration 5700/10000. Loss = 2.0296407736531217\n",
      "Iteration 5800/10000. Loss = 1.514823665842954\n",
      "Iteration 5900/10000. Loss = 2.0244003653154987\n",
      "Iteration 6000/10000. Loss = 2.2525925730674654\n",
      "Q dokn sosed. At she was modengs, and and gresss bon what he\n",
      "\n",
      "Iteration 6100/10000. Loss = 1.1740685416155292\n",
      "Iteration 6200/10000. Loss = 1.723645328641231\n",
      "Iteration 6300/10000. Loss = 1.4985475932762844\n",
      "Iteration 6400/10000. Loss = 1.5045376466294267\n",
      "Iteration 6500/10000. Loss = 2.0122330968539326\n",
      "Iteration 6600/10000. Loss = 1.9382170012388553\n",
      "Iteration 6700/10000. Loss = 1.6038239532309175\n",
      "Iteration 6800/10000. Loss = 2.8506494502220217\n",
      "Iteration 6900/10000. Loss = 2.0198407725898546\n",
      "Iteration 7000/10000. Loss = 1.858044016780594\n",
      "? in had for Mr. Buchad whe ciried suped, I and the wes veric\n",
      "Iteration 7100/10000. Loss = 1.541000834255561\n",
      "Iteration 7200/10000. Loss = 1.265415841939108\n",
      "Iteration 7300/10000. Loss = 1.8068608653437233\n",
      "Iteration 7400/10000. Loss = 2.126482746047989\n",
      "Iteration 7500/10000. Loss = 1.3459483773873826\n",
      "Iteration 7600/10000. Loss = 2.1435403967117534\n",
      "Iteration 7700/10000. Loss = 0.8587944357979902\n",
      "Iteration 7800/10000. Loss = 1.9953277964385034\n",
      "Iteration 7900/10000. Loss = 1.5381894533693652\n",
      "Iteration 8000/10000. Loss = 1.7072987401584023\n",
      "R that were the seeruag ackist nd wolliny.” chect very in bea\n",
      "Iteration 8100/10000. Loss = 2.3733445099155452\n",
      "Iteration 8200/10000. Loss = 1.9087627773132652\n",
      "Iteration 8300/10000. Loss = 1.4693424664774741\n",
      "Iteration 8400/10000. Loss = 1.6694961278822043\n",
      "Iteration 8500/10000. Loss = 2.083432042252671\n",
      "Iteration 8600/10000. Loss = 2.3487723079231784\n",
      "Iteration 8700/10000. Loss = 2.138217827405744\n",
      "Iteration 8800/10000. Loss = 1.6483054551399372\n",
      "Iteration 8900/10000. Loss = 1.9086137412937774\n",
      "Iteration 9000/10000. Loss = 2.3401340567643953\n",
      "xally the could it she whome\n",
      "      spelinces that his was not\n",
      "Iteration 9100/10000. Loss = 1.7717841806348886\n",
      "Iteration 9200/10000. Loss = 1.9353931025360196\n",
      "Iteration 9300/10000. Loss = 1.9144860960840404\n",
      "Iteration 9400/10000. Loss = 1.9616522348239696\n",
      "Iteration 9500/10000. Loss = 1.4066043215740254\n",
      "Iteration 9600/10000. Loss = 1.402619736885854\n",
      "Iteration 9700/10000. Loss = 2.116189677115678\n",
      "Iteration 9800/10000. Loss = 2.0515460822199607\n",
      "Iteration 9900/10000. Loss = 1.7731270096466045\n",
      "0.00686589399992954\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "tic = timeit.timeit()\n",
    "model = RNN(vocab_dim, idx_to_char, hidden_dim=100, input_dim=len(char_to_idx), cell_type='vanilla')\n",
    "solver = Solver(data, model,seq_length=25, lr=1e-1, num_iters=10000, batch_size = 1)\n",
    "solver.train()\n",
    "toc = timeit.timeit()\n",
    "print(toc-tic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
